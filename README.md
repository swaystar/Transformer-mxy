# Transformer ä½œä¸šå®ç°

æœ¬ä»“åº“å®ç°äº†ä¸€ä¸ªä»é›¶æ­å»ºçš„åŸºäº PyTorch çš„å°è§„æ¨¡ Transformer æ¨¡å‹ï¼Œæ”¯æŒï¼š
- âœ… **è¯­è¨€æ¨¡å‹ï¼ˆLanguage Modelï¼‰**ï¼šè‡ªå›å½’ç”Ÿæˆ
- âœ… **ç¼–ç å™¨-è§£ç å™¨ï¼ˆEncoder-Decoderï¼‰**ï¼šåºåˆ—åˆ°åºåˆ—ä»»åŠ¡
- âœ… **å¤šå¤´æ³¨æ„åŠ›**ï¼šæ ‡å‡†æ³¨æ„åŠ› + çº¿æ€§æ³¨æ„åŠ›ï¼ˆO(n) å¤æ‚åº¦ï¼‰
- âœ… **ä½ç½®ç¼–ç **ï¼šç»å¯¹ä½ç½®ç¼–ç  + ç›¸å¯¹ä½ç½®ç¼–ç 
- âœ… **å®Œæ•´è®­ç»ƒæµç¨‹**ï¼šå­¦ä¹ ç‡è°ƒåº¦ã€æ¢¯åº¦è£å‰ªã€AdamWã€æ¨¡å‹ä¿å­˜ã€è®­ç»ƒæ›²çº¿å¯è§†åŒ–

---

## ğŸ“‹ ç›®å½•ç»“æ„

```
Transformer/
â”œâ”€â”€ README.md                    # æœ¬æ–‡ä»¶
â”œâ”€â”€ requirements.txt             # Python ä¾èµ–åŒ…
â”œâ”€â”€ Description_of_the_Assignment.pdf  # ä½œä¸šè¦æ±‚æ–‡æ¡£
â”œâ”€â”€ src/                         # æºä»£ç ç›®å½•
â”‚   â”œâ”€â”€ __init__.py             # åŒ…åˆå§‹åŒ–
â”‚   â”œâ”€â”€ model.py                # Transformer æ¨¡å‹å®ç°ï¼ˆ833è¡Œï¼‰
â”‚   â”œâ”€â”€ data.py                 # æ•°æ®é›†åŠ è½½å’Œé¢„å¤„ç†ï¼ˆ223è¡Œï¼‰
â”‚   â”œâ”€â”€ train.py                # è®­ç»ƒå’ŒéªŒè¯è„šæœ¬ï¼ˆ500è¡Œï¼‰
â”‚   â””â”€â”€ utils.py                 # é…ç½®ç±»å’Œå·¥å…·å‡½æ•°ï¼ˆ97è¡Œï¼‰
â”œâ”€â”€ scripts/                     # è¿è¡Œè„šæœ¬
â”‚   â”œâ”€â”€ run.sh                  # Bash è„šæœ¬ï¼ˆLinux/macOS/WSLï¼‰
â”‚   â””â”€â”€ run.ps1                 # PowerShell è„šæœ¬ï¼ˆWindowsï¼‰
â””â”€â”€ results/                    # è®­ç»ƒç»“æœè¾“å‡ºç›®å½•
    â””â”€â”€ <exp_name>/
        â”œâ”€â”€ train_log.json      # è®­ç»ƒæ—¥å¿—ï¼ˆJSONæ ¼å¼ï¼‰
        â”œâ”€â”€ curves.png          # è®­ç»ƒæ›²çº¿å›¾ï¼ˆ4ä¸ªå­å›¾ï¼‰
        â”œâ”€â”€ model_best.pt       # æœ€ä¼˜æ¨¡å‹æƒé‡
        â””â”€â”€ model_last.pt       # æœ€æ–°æ¨¡å‹æƒé‡
```

---

## ğŸ–¥ï¸ ç¡¬ä»¶è¦æ±‚

### **æœ€ä½è¦æ±‚**
- **CPU**ï¼šæ”¯æŒ Python 3.8+ çš„ç°ä»£ CPU
- **å†…å­˜**ï¼šè‡³å°‘ 4GB RAM
- **ç£ç›˜ç©ºé—´**ï¼šè‡³å°‘ 2GBï¼ˆç”¨äºæ•°æ®é›†ã€æ¨¡å‹å’Œç»“æœï¼‰

### **æ¨èé…ç½®**
- **CPU**ï¼šå¤šæ ¸ CPUï¼ˆæ¨è 4 æ ¸ä»¥ä¸Šï¼‰
- **GPU**ï¼ˆå¯é€‰ä½†å¼ºçƒˆæ¨èï¼‰ï¼š
  - NVIDIA GPU with CUDA æ”¯æŒ
  - è‡³å°‘ 4GB æ˜¾å­˜ï¼ˆæ¨è 8GB+ï¼‰
  - CUDA 11.0 æˆ–æ›´é«˜ç‰ˆæœ¬
- **å†…å­˜**ï¼š8GB+ RAM
- **ç£ç›˜ç©ºé—´**ï¼š5GB+ï¼ˆç”¨äºå®Œæ•´è®­ç»ƒè¿‡ç¨‹å’Œç»“æœï¼‰

### **è®¾å¤‡è‡ªåŠ¨æ£€æµ‹**
ä»£ç ä¼šè‡ªåŠ¨æ£€æµ‹å¯ç”¨è®¾å¤‡ï¼š
- ä¼˜å…ˆä½¿ç”¨ **GPU**ï¼ˆå¦‚æœå¯ç”¨ï¼‰
- å¦åˆ™ä½¿ç”¨ **CPU**
- æ”¯æŒ CUDAã€MPSï¼ˆApple Siliconï¼‰å’Œ CPU

### **æ€§èƒ½å‚è€ƒ**
- **CPU è®­ç»ƒ**ï¼šçº¦ 0.5-2 step/sï¼ˆå–å†³äº CPU æ€§èƒ½ï¼‰
- **GPU è®­ç»ƒ**ï¼šçº¦ 10-50+ step/sï¼ˆå–å†³äº GPU å‹å·ï¼‰
- **å•æ¬¡å®éªŒæ—¶é•¿**ï¼ˆ5000 stepsï¼‰ï¼š
  - CPUï¼šçº¦ 40-200 åˆ†é’Ÿ
  - GPUï¼šçº¦ 2-10 åˆ†é’Ÿ

---

## ğŸ”§ ç¯å¢ƒå®‰è£…

### **1. Python ç¯å¢ƒ**
ç¡®ä¿å·²å®‰è£… Python 3.8 æˆ–æ›´é«˜ç‰ˆæœ¬ï¼š
```bash
python --version  # åº”æ˜¾ç¤º 3.8+
```

### **2. åˆ›å»ºè™šæ‹Ÿç¯å¢ƒï¼ˆæ¨èï¼‰**
```bash
# Windows
python -m venv .venv
.venv\Scripts\activate

# Linux/macOS/WSL
python -m venv .venv
source .venv/bin/activate
```

### **3. å®‰è£…ä¾èµ–**
```bash
pip install -r requirements.txt
```

### **4. éªŒè¯å®‰è£…**
```bash
python -c "import torch; print(f'PyTorchç‰ˆæœ¬: {torch.__version__}'); print(f'CUDAå¯ç”¨: {torch.cuda.is_available()}')"
```

---

## ğŸš€ å¿«é€Ÿå¼€å§‹

### **æ–¹å¼ä¸€ï¼šä½¿ç”¨ä¸€é”®è¿è¡Œè„šæœ¬**

#### **Windows PowerShell**
```powershell
powershell -ExecutionPolicy Bypass -File scripts/run.ps1
```

#### **Linux/macOS/WSL Bash**
```bash
bash scripts/run.sh
```

### **æ–¹å¼äºŒï¼šç›´æ¥è¿è¡Œ Python å‘½ä»¤**
```bash
python -m src.train
```

---

## ğŸ“ å¯å¤ç°å®éªŒçš„ Exact å‘½ä»¤ï¼ˆå«éšæœºç§å­ï¼‰

### **âš ï¸ é‡è¦ï¼šä»¥ä¸‹å‘½ä»¤åŒ…å«å›ºå®šéšæœºç§å­ï¼Œå¯å®Œå…¨å¤ç°å®éªŒç»“æœ**

### **1. åŸºç¡€è¯­è¨€æ¨¡å‹ï¼ˆæ¨èé…ç½®ï¼‰**

```bash
python -m src.train \
  --dataset tiny_shakespeare \
  --vocab byte \
  --seq_len 256 \
  --d_model 256 \
  --n_heads 4 \
  --n_layers 4 \
  --ffn_hidden 1024 \
  --dropout 0.1 \
  --model_type lm \
  --attention_type standard \
  --pos_encoding_type absolute \
  --lr 3e-4 \
  --optimizer adamw \
  --weight_decay 0.01 \
  --scheduler cosine \
  --warmup_steps 500 \
  --batch_size 64 \
  --max_steps 5000 \
  --eval_interval 200 \
  --seed 3407 \
  --results_dir results/exp_lm_standard_abs_seed3407
```

**é¢„æœŸç»“æœ**ï¼š
- è®­ç»ƒæ—¶é—´ï¼šçº¦ 5-30 åˆ†é’Ÿï¼ˆå–å†³äºç¡¬ä»¶ï¼‰
- æœ€ç»ˆéªŒè¯æŸå¤±ï¼šçº¦ 1.5-2.5
- æœ€ç»ˆå›°æƒ‘åº¦ï¼šçº¦ 5-15

---

### **2. è¯­è¨€æ¨¡å‹ + ç›¸å¯¹ä½ç½®ç¼–ç **

```bash
python -m src.train \
  --dataset tiny_shakespeare \
  --vocab byte \
  --seq_len 256 \
  --d_model 256 \
  --n_heads 4 \
  --n_layers 4 \
  --ffn_hidden 1024 \
  --dropout 0.1 \
  --model_type lm \
  --attention_type standard \
  --pos_encoding_type relative \
  --max_relative_position 128 \
  --lr 3e-4 \
  --optimizer adamw \
  --weight_decay 0.01 \
  --scheduler cosine \
  --warmup_steps 500 \
  --batch_size 64 \
  --max_steps 5000 \
  --eval_interval 200 \
  --seed 3407 \
  --results_dir results/exp_lm_standard_rel_seed3407
```

---

### **3. è¯­è¨€æ¨¡å‹ + çº¿æ€§æ³¨æ„åŠ›**

```bash
python -m src.train \
  --dataset tiny_shakespeare \
  --vocab byte \
  --seq_len 256 \
  --d_model 256 \
  --n_heads 4 \
  --n_layers 4 \
  --ffn_hidden 1024 \
  --dropout 0.1 \
  --model_type lm \
  --attention_type linear \
  --pos_encoding_type absolute \
  --lr 3e-4 \
  --optimizer adamw \
  --weight_decay 0.01 \
  --scheduler cosine \
  --warmup_steps 500 \
  --batch_size 64 \
  --max_steps 5000 \
  --eval_interval 200 \
  --seed 3407 \
  --results_dir results/exp_lm_linear_abs_seed3407
```

---

### **4. ç¼–ç å™¨-è§£ç å™¨æ¨¡å‹ï¼ˆSeq2Seqï¼‰**

```bash
python -m src.train \
  --dataset tiny_shakespeare \
  --vocab byte \
  --seq_len 256 \
  --d_model 256 \
  --n_heads 4 \
  --n_encoder_layers 4 \
  --n_decoder_layers 4 \
  --ffn_hidden 1024 \
  --dropout 0.1 \
  --model_type seq2seq \
  --attention_type standard \
  --pos_encoding_type absolute \
  --lr 3e-4 \
  --optimizer adamw \
  --weight_decay 0.01 \
  --scheduler cosine \
  --warmup_steps 500 \
  --batch_size 64 \
  --max_steps 5000 \
  --eval_interval 200 \
  --seed 3407 \
  --results_dir results/exp_seq2seq_standard_abs_seed3407
```

---

### **5. å®Œæ•´æŒ‘æˆ˜é…ç½®ï¼ˆç›¸å¯¹ä½ç½®ç¼–ç  + çº¿æ€§æ³¨æ„åŠ› + è§£ç å™¨ï¼‰**

```bash
python -m src.train \
  --dataset tiny_shakespeare \
  --vocab byte \
  --seq_len 256 \
  --d_model 256 \
  --n_heads 4 \
  --n_encoder_layers 4 \
  --n_decoder_layers 4 \
  --ffn_hidden 1024 \
  --dropout 0.1 \
  --model_type seq2seq \
  --attention_type linear \
  --pos_encoding_type relative \
  --max_relative_position 128 \
  --lr 3e-4 \
  --optimizer adamw \
  --weight_decay 0.01 \
  --scheduler cosine \
  --warmup_steps 500 \
  --batch_size 64 \
  --max_steps 5000 \
  --eval_interval 200 \
  --seed 3407 \
  --results_dir results/exp_seq2seq_linear_rel_seed3407
```

---

## âš™ï¸ å‘½ä»¤è¡Œå‚æ•°è¯´æ˜

### **æ•°æ®é›†å‚æ•°**
- `--dataset`: æ•°æ®é›†åç§°ï¼ˆ`tiny_shakespeare`ï¼‰
- `--vocab`: è¯æ±‡è¡¨ç±»å‹ï¼ˆ`byte` æˆ– `char`ï¼‰
- `--seq_len`: åºåˆ—é•¿åº¦ï¼ˆé»˜è®¤ `256`ï¼‰

### **æ¨¡å‹æ¶æ„å‚æ•°**
- `--d_model`: æ¨¡å‹ç»´åº¦ï¼ˆé»˜è®¤ `256`ï¼‰
- `--n_heads`: æ³¨æ„åŠ›å¤´æ•°é‡ï¼ˆé»˜è®¤ `4`ï¼‰
- `--n_layers`: Transformer å±‚æ•°ï¼ˆè¯­è¨€æ¨¡å‹ï¼Œé»˜è®¤ `4`ï¼‰
- `--n_encoder_layers`: ç¼–ç å™¨å±‚æ•°ï¼ˆSeq2Seqï¼Œé»˜è®¤ `4`ï¼‰
- `--n_decoder_layers`: è§£ç å™¨å±‚æ•°ï¼ˆSeq2Seqï¼Œé»˜è®¤ `4`ï¼‰
- `--ffn_hidden`: å‰é¦ˆç½‘ç»œéšè—å±‚ç»´åº¦ï¼ˆé»˜è®¤ `1024`ï¼‰
- `--dropout`: Dropout æ¯”ç‡ï¼ˆé»˜è®¤ `0.1`ï¼‰

### **æ¨¡å‹ç±»å‹å‚æ•°ï¼ˆæ–°å¢ï¼‰**
- `--model_type`: æ¨¡å‹ç±»å‹
  - `lm`: è¯­è¨€æ¨¡å‹ï¼ˆè‡ªå›å½’ï¼‰
  - `seq2seq`: ç¼–ç å™¨-è§£ç å™¨
- `--attention_type`: æ³¨æ„åŠ›æœºåˆ¶ç±»å‹
  - `standard`: æ ‡å‡†ç¼©æ”¾ç‚¹ç§¯æ³¨æ„åŠ›ï¼ˆO(nÂ²)ï¼‰
  - `linear`: çº¿æ€§æ³¨æ„åŠ›ï¼ˆO(n)ï¼Œé€‚åˆé•¿åºåˆ—ï¼‰
- `--pos_encoding_type`: ä½ç½®ç¼–ç ç±»å‹
  - `absolute`: ç»å¯¹ä½ç½®ç¼–ç ï¼ˆæ­£å¼¦/ä½™å¼¦ï¼‰
  - `relative`: ç›¸å¯¹ä½ç½®ç¼–ç ï¼ˆShaw et al.ï¼‰
- `--max_relative_position`: æœ€å¤§ç›¸å¯¹ä½ç½®è·ç¦»ï¼ˆä»…ç”¨äºç›¸å¯¹ä½ç½®ç¼–ç ï¼Œé»˜è®¤ `128`ï¼‰

### **è®­ç»ƒè¶…å‚æ•°**
- `--lr`: å­¦ä¹ ç‡ï¼ˆé»˜è®¤ `3e-4`ï¼‰
- `--optimizer`: ä¼˜åŒ–å™¨ï¼ˆ`adamw` æˆ– `adam`ï¼Œé»˜è®¤ `adamw`ï¼‰
- `--weight_decay`: æƒé‡è¡°å‡ï¼ˆé»˜è®¤ `0.01`ï¼‰
- `--scheduler`: å­¦ä¹ ç‡è°ƒåº¦ç­–ç•¥
  - `cosine`: ä½™å¼¦é€€ç« + çº¿æ€§ warmupï¼ˆæ¨èï¼‰
  - `onecycle`: å•å‘¨æœŸå­¦ä¹ ç‡
  - `steplr`: é˜¶æ¢¯å¼è¡°å‡
  - `none`: å›ºå®šå­¦ä¹ ç‡
- `--warmup_steps`: Warmup æ­¥æ•°ï¼ˆé»˜è®¤ `500`ï¼‰

### **è®­ç»ƒè®¾ç½®**
- `--batch_size`: æ‰¹æ¬¡å¤§å°ï¼ˆé»˜è®¤ `64`ï¼‰
- `--max_steps`: æœ€å¤§è®­ç»ƒæ­¥æ•°ï¼ˆé»˜è®¤ `5000`ï¼‰
- `--eval_interval`: è¯„ä¼°é—´éš”ï¼ˆæ­¥æ•°ï¼Œé»˜è®¤ `200`ï¼‰
- `--seed`: **éšæœºç§å­**ï¼ˆé»˜è®¤ `3407`ï¼Œç”¨äºå¤ç°å®éªŒï¼‰

### **è¾“å‡ºè®¾ç½®**
- `--results_dir`: ç»“æœä¿å­˜ç›®å½•ï¼ˆé»˜è®¤ `results/exp_default`ï¼‰
- `--device`: æŒ‡å®šè®¾å¤‡ï¼ˆ`cpu` æˆ– `cuda`ï¼Œé»˜è®¤ `None` è‡ªåŠ¨æ£€æµ‹ï¼‰

---

## ğŸ“Š è¾“å‡ºæ–‡ä»¶è¯´æ˜

è®­ç»ƒå®Œæˆåï¼Œåœ¨ `results/<exp_name>/` ç›®å½•ä¸‹ä¼šç”Ÿæˆï¼š

### **1. train_log.json**
è®­ç»ƒå’ŒéªŒè¯æ—¥å¿—ï¼ŒåŒ…å«æ¯æ¬¡è¯„ä¼°çš„ï¼š
- `step`: è®­ç»ƒæ­¥æ•°
- `train_loss`: è®­ç»ƒæŸå¤±
- `eval_loss`: éªŒè¯æŸå¤±
- `eval_ppl`: éªŒè¯å›°æƒ‘åº¦ï¼ˆPerplexityï¼‰
- `lr`: å½“å‰å­¦ä¹ ç‡

### **2. curves.png**
è¯¦ç»†çš„è®­ç»ƒæ›²çº¿å›¾ï¼ˆ150 DPIï¼‰ï¼ŒåŒ…å« 4 ä¸ªå­å›¾ï¼š
- ğŸ“Š **è®­ç»ƒ/éªŒè¯ Loss æ›²çº¿**ï¼šå¯¹æ¯”è®­ç»ƒå’ŒéªŒè¯æŸå¤±çš„å˜åŒ–
- ğŸ“ˆ **éªŒè¯é›† Perplexity æ›²çº¿**ï¼šå›°æƒ‘åº¦ä¸‹é™è¶‹åŠ¿
- ğŸ“‰ **Learning Rate è°ƒåº¦æ›²çº¿**ï¼šå­¦ä¹ ç‡å˜åŒ–ï¼ˆå¯¹æ•°åˆ»åº¦ï¼‰
- ğŸ” **è¿‡æ‹ŸåˆæŒ‡ç¤ºå™¨**ï¼šè®­ç»ƒæŸå¤± - éªŒè¯æŸå¤±ï¼ˆç”¨äºåˆ¤æ–­è¿‡æ‹Ÿåˆï¼‰

### **3. model_best.pt**
éªŒè¯é›†ä¸Šè¡¨ç°æœ€å¥½çš„æ¨¡å‹æƒé‡ï¼ˆPyTorch state_dict æ ¼å¼ï¼‰

### **4. model_last.pt**
æœ€åä¸€æ¬¡ä¿å­˜çš„æ¨¡å‹æƒé‡

---

## ğŸ” å•ç‹¬ç»˜åˆ¶è®­ç»ƒæ›²çº¿

å¦‚æœå·²æœ‰è®­ç»ƒæ—¥å¿—ï¼Œå¯ä»¥å•ç‹¬ç”Ÿæˆå›¾è¡¨ï¼š

```bash
python -m src.train --plot results/exp_default/train_log.json

# æˆ–æŒ‡å®šè¾“å‡ºè·¯å¾„
python -m src.train --plot results/exp_default/train_log.json output.png
```

---

## ğŸ› å¸¸è§é—®é¢˜

### **1. Windows OpenMP é”™è¯¯**
å¦‚æœé‡åˆ°ä»¥ä¸‹é”™è¯¯ï¼š
```
OMP: Error #15: Initializing libiomp5md.dll, but found libiomp5md.dll already initialized.
```

**è§£å†³æ–¹æ¡ˆ**ï¼š
- âœ… **å·²è‡ªåŠ¨ä¿®å¤**ï¼šä»£ç ä¸­å·²åœ¨å¯¼å…¥ torch ä¹‹å‰è®¾ç½® `KMP_DUPLICATE_LIB_OK=TRUE`
- å¦‚æœä»æœ‰é—®é¢˜ï¼Œå¯æ‰‹åŠ¨è®¾ç½®ï¼š
  ```powershell
  $env:KMP_DUPLICATE_LIB_OK="TRUE"
  python -m src.train ...
  ```

### **2. æ•°æ®é›†ä¸‹è½½å¤±è´¥**
**å¯èƒ½åŸå› **ï¼šç½‘ç»œè¿æ¥é—®é¢˜ï¼Œæ— æ³•è®¿é—® GitHub

**è§£å†³æ–¹æ¡ˆ**ï¼š
1. æ£€æŸ¥ç½‘ç»œè¿æ¥
2. æ‰‹åŠ¨ä¸‹è½½æ•°æ®é›†ï¼š
   - ä¸‹è½½åœ°å€ï¼šhttps://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt
   - ä¿å­˜åˆ°ï¼š`.cache/tiny_shakespeare/tinyshakespeare.txt`
3. é‡æ–°è¿è¡Œç¨‹åº

### **3. CUDA å†…å­˜ä¸è¶³**
**è§£å†³æ–¹æ¡ˆ**ï¼š
- å‡å°æ‰¹æ¬¡å¤§å°ï¼š`--batch_size 32`
- å‡å°æ¨¡å‹å¤§å°ï¼š`--d_model 128` æˆ– `--n_layers 2`
- ä½¿ç”¨ CPUï¼š`--device cpu`

### **4. å¯¼å…¥é”™è¯¯ï¼ˆImportErrorï¼‰**
**è§£å†³æ–¹æ¡ˆ**ï¼š
- ç¡®ä¿ä»é¡¹ç›®æ ¹ç›®å½•è¿è¡Œï¼š`python -m src.train`
- ç¡®ä¿å·²å®‰è£…æ‰€æœ‰ä¾èµ–ï¼š`pip install -r requirements.txt`
- æ£€æŸ¥ Python è·¯å¾„æ˜¯å¦æ­£ç¡®

---

## ğŸ“– ä»£ç ç»“æ„è¯´æ˜

### **æ ¸å¿ƒæ¨¡å—**

#### **src/model.py**
- `PositionalEncoding`: ç»å¯¹ä½ç½®ç¼–ç ï¼ˆæ­£å¼¦/ä½™å¼¦ï¼‰
- `RelativePositionalEncoding`: ç›¸å¯¹ä½ç½®ç¼–ç 
- `LinearAttention`: çº¿æ€§æ³¨æ„åŠ›ï¼ˆO(n) å¤æ‚åº¦ï¼‰
- `MultiHeadSelfAttention`: æ ‡å‡†å¤šå¤´è‡ªæ³¨æ„åŠ›
- `MultiHeadCrossAttention`: äº¤å‰æ³¨æ„åŠ›ï¼ˆç”¨äºè§£ç å™¨ï¼‰
- `TransformerBlock`: ç¼–ç å™¨å—
- `DecoderBlock`: è§£ç å™¨å—
- `TransformerLM`: è¯­è¨€æ¨¡å‹
- `TransformerEncoderDecoder`: ç¼–ç å™¨-è§£ç å™¨æ¨¡å‹

#### **src/train.py**
- `parse_args()`: è§£æå‘½ä»¤è¡Œå‚æ•°
- `build_optimizer()`: æ„å»ºä¼˜åŒ–å™¨
- `build_scheduler()`: æ„å»ºå­¦ä¹ ç‡è°ƒåº¦å™¨
- `evaluate()`: è¯„ä¼°æ¨¡å‹æ€§èƒ½
- `plot_curves()`: ç»˜åˆ¶è®­ç»ƒæ›²çº¿
- `main()`: ä¸»è®­ç»ƒå¾ªç¯

#### **src/data.py**
- `CharByteTokenizer`: å­—ç¬¦/å­—èŠ‚çº§åˆ†è¯å™¨
- `LMSequenceDataset`: è¯­è¨€æ¨¡å‹æ•°æ®é›†
- `load_dataset()`: åŠ è½½æ•°æ®é›†å¹¶åˆ›å»º DataLoader

---

## ğŸ¯ å®éªŒå¤ç°å»ºè®®

### **æ¨èçš„å®éªŒé…ç½®**

1. **åŸºç¡€å®éªŒï¼ˆå¿«é€ŸéªŒè¯ï¼‰**ï¼š
   ```bash
   python -m src.train --max_steps 1000 --batch_size 32 --seed 3407
   ```

2. **å®Œæ•´å®éªŒï¼ˆä½œä¸šè¦æ±‚ï¼‰**ï¼š
   ä½¿ç”¨ä¸Šé¢æä¾›çš„ exact å‘½ä»¤ï¼ˆåŒ…å«å®Œæ•´å‚æ•°å’Œéšæœºç§å­ï¼‰

3. **å¯¹æ¯”å®éªŒï¼ˆæ¶ˆèç ”ç©¶ï¼‰**ï¼š
   - è¿è¡Œä¸åŒçš„é…ç½®ï¼ˆæ ‡å‡†/çº¿æ€§æ³¨æ„åŠ›ã€ç»å¯¹/ç›¸å¯¹ä½ç½®ç¼–ç ï¼‰
   - æ¯”è¾ƒ `train_log.json` ä¸­çš„ç»“æœ
   - æŸ¥çœ‹ `curves.png` çš„æ›²çº¿å¯¹æ¯”

### **ç¡®ä¿å¯å¤ç°æ€§**
- âœ… **å›ºå®šéšæœºç§å­**ï¼šæ‰€æœ‰å‘½ä»¤éƒ½åŒ…å« `--seed 3407`
- âœ… **å›ºå®šè¶…å‚æ•°**ï¼šä½¿ç”¨å‘½ä»¤ä¸­æŒ‡å®šçš„æ‰€æœ‰å‚æ•°
- âœ… **ç›¸åŒç¯å¢ƒ**ï¼šä½¿ç”¨ç›¸åŒçš„ Python å’Œ PyTorch ç‰ˆæœ¬

---

## ğŸ“š å‚è€ƒèµ„æ–™

- Vaswani et al., "Attention Is All You Need" (2017)
- Shaw et al., "Self-Attention with Relative Position Representations" (2018)
- Katharopoulos et al., "Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention" (2020)

---

## ğŸ“„ è®¸å¯è¯

æœ¬é¡¹ç›®ä¸ºä½œä¸šå®ç°ï¼Œä»…ä¾›å­¦ä¹ å’Œç ”ç©¶ä½¿ç”¨ã€‚

---

**æœ€åæ›´æ–°**ï¼š2024å¹´
**ç»´æŠ¤è€…**ï¼šTransformer ä½œä¸šå®ç°

